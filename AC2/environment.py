import numpy as np
import enum
from DQN.lib.DataFeature import DataFeature
from utils.AppSetting import AppSetting
import time

class Actions(enum.Enum):
    No_hold = 0 
    Buy = 1

    
class State:
    def __init__(self, bars_count:int, reset_on_close:bool):
        """
            bars_count(int):
            ç”¨ä¾†çµ„æˆç¥ç¶“ç¶²çµ¡æ‰€éœ€è¦çš„ç’°å¢ƒ

        """
        self.bars_count = bars_count # æ‰€éœ€è¦ä½¿ç”¨çš„Kæ£’æ•¸é‡
        self.setting = AppSetting.get_ooas_setting()
        self.reset_on_close = reset_on_close
        self.alpha =0.0
        
    @property
    def shape(self):
        """        
            æ ¹æ“šä½ çš„ç‰¹å¾µä¾†æ±ºå®šshape
        """
        return (4 * self.bars_count + 1, )

    
    def reset(self,prices,offset):
        """
            é€éreset ä¾†æŒ‡å®šéœ€è¦çš„æ¨™çš„ç‰©
        """
        self._prices = prices
        self._offset = offset
        self.current_action = None
        self.update_scale()
    
    def get_dataFeature_shape(self,model:str) ->int:        
        if model == 'RNN':
            # [high,low,close,volume,position]
            return 5
        
    def encode(self):
        """
            [batch_size,seqlength,input_size] (RNNçš„è¼¸å…¥æ¨¡å‹ç‚ºæ­¤)
            
            åœ¨é€™å€‹function è£¡é¢æˆ‘èªç‚º,
                åªéœ€è¦æº–å‚™ [seqlength,input_size]
        """        
        res = np.ndarray(shape= [self.bars_count,self.get_dataFeature_shape(model='RNN')], dtype=np.float32)
        shift = 0
        for bar_idx in range(-self.bars_count, 0):            
            res[shift,0] = self._prices.high[self._offset + bar_idx]            
            res[shift,1] = self._prices.low[self._offset + bar_idx]            
            res[shift,2] = self._prices.close[self._offset + bar_idx]            
            res[shift,3] = self._prices.volume[self._offset + bar_idx]
            res[shift,4] = 0.0 if self.current_action is None else self.current_action
            shift += 1
        
        # ç¢ºå®šè«–æ–‡è£¡é¢æ˜¯æœ‰å°‡éƒ¨ä½ç‹€æ…‹æ”¾å…¥çš„        
        return res
    
    def _cur_close(self):
        """
        Calculate real close price for the current bar

        # ç‚ºç”šéº¼æœƒé€™æ¨£å¯«çš„åŸå› æ˜¯å› ç‚º é€érel_close ç´€éŒ„çš„å’Œopen price çš„å·®è·(ç™¾åˆ†æ¯”)ä¾†å–å¾—çœŸå¯¦çš„æ”¶ç›¤åƒ¹
        """
        open = self._prices.open[self._offset]
        rel_close = self._prices.close[self._offset]
        return open * (1.0 + rel_close)
    
    def update_scale(self):
        """
            ç‚ºäº†è¦å°‡æ”¶ç›¤åƒ¹æ ¼æ­£å‰‡åŒ–
            æ‰€ä»¥è¦çŸ¥é“æœ€é«˜æœ€ä½åƒ¹
        """
        
        close_array = self._prices.open* (1+self._prices.close)
        self.maxprice = max(close_array)
        self.minprice = min(close_array)
    
    def normalize_price(self, price):
        """
        å¯¹ç»™å®šçš„ä»·æ ¼è¿›è¡Œè§„èŒƒåŒ–ã€‚
        
        å‚æ•°:
        price (float): è¦è§„èŒƒåŒ–çš„ä»·æ ¼ã€‚

        è¿”å›:
        float: è§„èŒƒåŒ–åçš„ä»·æ ¼ã€‚
        """
        return (price - self.minprice) / (self.maxprice - self.minprice)
    
    def calculate_reward(self,sa_current, sa_next, price, alpha):
        """
            1.å®šä¹‰å¥–åŠ±å‡½æ•°:
                å½“ä»£ç†å†³å®šä¹°å…¥æ—¶ï¼Œä»£ç†èŠ±è´¹çš„é’±å°†å¢åŠ ï¼Œå› æ­¤å¥–åŠ±ä¸ºè´Ÿå€¼ã€‚
                å½“ä»£ç†å†³å®šå–å‡ºæ—¶ï¼Œä»£ç†è·å¾—çš„é’±å°†å¢åŠ ï¼Œå› æ­¤å¥–åŠ±ä¸ºæ­£å€¼ã€‚                
                
            2.äº¤æ˜“æˆæœ¬:
                äº¤æ˜“æˆæœ¬é€šè¿‡äº¤æ˜“é‡‘é¢å’Œæˆæœ¬æ¯”ç‡çš„ä¹˜ç§¯è®¡ç®—ã€‚
                
        
        è®¡ç®—å¥–åŠ±å‡½æ•°ã€‚

        å‚æ•°:
        sa_current: å½“å‰æ—¶é—´æ­¥çš„ä»£ç†ä½ç½®çŠ¶æ€ã€‚
        sa_next: ä¸‹ä¸€æ—¶é—´æ­¥çš„ä»£ç†ä½ç½®çŠ¶æ€ã€‚
        p_buy: å½“å‰æ—¶é—´æ­¥çš„ä¹°å…¥ä»·æ ¼ã€‚
        p_sell: å½“å‰æ—¶é—´æ­¥çš„å–å‡ºä»·æ ¼ã€‚
        alpha_buy: è²·å…¥æ™‚çš„äº¤æ˜“æˆæœ¬æ¯”ç‡ã€‚ (æ‰‹çºŒè²»,äº¤æ˜“ç¨…,æ»‘åƒ¹)
        alpha_sell: å–å‡ºæ—¶çš„äº¤æ˜“æˆæœ¬æ¯”ç‡ã€‚

        è¿”å›:
        å¥–åŠ±å€¼ã€‚
        """
        if sa_next > sa_current:
            reward = -abs(sa_next - sa_current) * (1 + alpha) * price
        else:
            reward = 2 * abs(sa_next - sa_current) * (1 - alpha) * price

        return reward
    
    def step(self, action):
        """
            é‡æ–°å»ºæ§‹çå‹µ:
                æ¡ç”¨æ–°çš„å…¬å¼å˜—è©¦çœ‹çœ‹
                                
            sa_current: å½“å‰æ—¶é—´æ­¥çš„ä»£ç†ä½ç½®çŠ¶æ€ã€‚
            sa_next: ä¸‹ä¸€æ—¶é—´æ­¥çš„ä»£ç†ä½ç½®çŠ¶æ€ã€‚
            p_buy: å½“å‰æ—¶é—´æ­¥çš„ä¹°å…¥ä»·æ ¼ã€‚
            p_sell: å½“å‰æ—¶é—´æ­¥çš„å–å‡ºä»·æ ¼ã€‚
            alpha_buy: è²·å…¥æ™‚çš„äº¤æ˜“æˆæœ¬æ¯”ç‡ã€‚ (æ‰‹çºŒè²»,äº¤æ˜“ç¨…,æ»‘åƒ¹)
            alpha_sell: å–å‡ºæ—¶çš„äº¤æ˜“æˆæœ¬æ¯”ç‡ã€‚
        """
        assert isinstance(action, Actions)
        
        reward = 0.0
        done = False
        close = self._cur_close()
        sa_current = 0.0 if self.current_action is None else self.current_action
        
        if action == Actions.No_hold:       
            sa_next = 0.0 #  if ğ‘ğ‘– = 1,then ğ‘ ğ‘ğ‘–+1 = 1 at the next time step.
        elif action == Actions.Buy:
            sa_next = 1.0

        self._offset += 1        
        prev_close = close # ä¸Šä¸€æ ¹çš„æ”¶ç›¤åƒ¹
        close = self._cur_close()
        done |= self._offset >= self._prices.close.shape[0]-1        
        self.current_action = sa_next
        reward = self.calculate_reward(sa_current=sa_current,
                              sa_next=sa_next,
                              price = self.normalize_price(close),
                              alpha=self.alpha,
        )
        
        # print("ç•¶å‰å‹•ä½œ:",action,"ç›®å‰éƒ¨ä½:",sa_current,"ä¸‹ä¸€å€‹å‹•ä½œ:",sa_next,"æ­£å‰‡åŒ–åƒ¹æ ¼",self.normalize_price(close),"ç›®å‰çå‹µ:",reward)
        return reward, done
    
    
    def update_alpha(self,step_idx):
        # æ¯æ¬¡è°ƒç”¨æ—¶é€æ­¥å¢åŠ alphaå€¼ï¼Œä½†ä¸è¶…è¿‡æœ€å¤§å€¼
        self.alpha = min(step_idx / 1000000, self.setting['MODEL_DEFAULT_COMMISSION_PERC'] +self.setting['DEFAULT_SLIPPAGE'] )
    
class Simulate_env():
    def __init__(self,all_prices:dict) -> None:
        """
        ç”¨ä¾†æ¨¡æ“¬
            replayä¸­çš„rewardè¨ˆç®—        
        """
        self.all_prices  = all_prices
        self.setting = AppSetting.get_ooas_setting()
         
    def asign(self,state,info:dict):
        """
        Returns:
            info: {'instrument': 'BTCUSDT', 'offset': 68638}
        
        """
        assert isinstance(info,dict) == True,"info must be dict"
        
        self.instrument = info['instrument']
        self._offset = info['offset']

        # æ›´æ–°è‡³æŒ‡å®šå•†å“
        self._prices = self.all_prices[self.instrument]

        
        if state[-1] == 0:
            self.have_position = False
        elif state[-1] == 1:
            self.have_position = True
        
    def _cur_close(self):
        """
        Calculate real close price for the current bar

        # ç‚ºç”šéº¼æœƒé€™æ¨£å¯«çš„åŸå› æ˜¯å› ç‚º é€érel_close ç´€éŒ„çš„å’Œopen price çš„å·®è·(ç™¾åˆ†æ¯”)ä¾†å–å¾—çœŸå¯¦çš„æ”¶ç›¤åƒ¹
        """
        open = self._prices.open[self._offset]
        rel_close = self._prices.close[self._offset]
        return open * (1.0 + rel_close)    
    
    def step(self, action):
        """
            é‡æ–°å»ºæ§‹çå‹µ:
                æ¡ç”¨æ–°çš„å…¬å¼å˜—è©¦çœ‹çœ‹
        """
        assert isinstance(action, Actions)
        reward = 0.0
        done = False
        close = self._cur_close()
        
        sa_current = float(self.have_position)
        
        if action == Actions.Buy and not self.have_position:
            self.have_position = True           
            self.open_price = close * (1 + self.setting['DEFAULT_SLIPPAGE'])
            
        elif action == Actions.Close and self.have_position:
            self.have_position = False
            self.open_price = 0.0
                
        self._offset += 1        
        prev_close = close # ä¸Šä¸€æ ¹çš„æ”¶ç›¤åƒ¹
        close = self._cur_close()
        done |= self._offset >= self._prices.close.shape[0]-1        
    
        reward = self.calculate_reward(sa_current=sa_current,
                              sa_next=float(self.have_position),
                              p_buy=prev_close,
                              p_sell=close,
                              alpha_buy=self.setting['MODEL_DEFAULT_COMMISSION_PERC'],
                              alpha_sell=self.setting['MODEL_DEFAULT_COMMISSION_PERC'] +self.setting['DEFAULT_SLIPPAGE'] )
        
        
        return reward, done        
    
    def calculate_reward(self,sa_current, sa_next, p_buy, p_sell, alpha_buy, alpha_sell):
        """
            1.å®šä¹‰å¥–åŠ±å‡½æ•°:
                å½“ä»£ç†å†³å®šä¹°å…¥æ—¶ï¼Œä»£ç†èŠ±è´¹çš„é’±å°†å¢åŠ ï¼Œå› æ­¤å¥–åŠ±ä¸ºè´Ÿå€¼ã€‚
                å½“ä»£ç†å†³å®šå–å‡ºæ—¶ï¼Œä»£ç†è·å¾—çš„é’±å°†å¢åŠ ï¼Œå› æ­¤å¥–åŠ±ä¸ºæ­£å€¼ã€‚
                
                
            2.äº¤æ˜“æˆæœ¬:
                äº¤æ˜“æˆæœ¬é€šè¿‡äº¤æ˜“é‡‘é¢å’Œæˆæœ¬æ¯”ç‡çš„ä¹˜ç§¯è®¡ç®—ã€‚
                
        
        è®¡ç®—å¥–åŠ±å‡½æ•°ã€‚

        å‚æ•°:
        sa_current: å½“å‰æ—¶é—´æ­¥çš„ä»£ç†ä½ç½®çŠ¶æ€ã€‚
        sa_next: ä¸‹ä¸€æ—¶é—´æ­¥çš„ä»£ç†ä½ç½®çŠ¶æ€ã€‚
        p_buy: å½“å‰æ—¶é—´æ­¥çš„ä¹°å…¥ä»·æ ¼ã€‚
        p_sell: å½“å‰æ—¶é—´æ­¥çš„å–å‡ºä»·æ ¼ã€‚
        alpha_buy: è²·å…¥æ™‚çš„äº¤æ˜“æˆæœ¬æ¯”ç‡ã€‚ (æ‰‹çºŒè²»,äº¤æ˜“ç¨…,æ»‘åƒ¹)
        alpha_sell: å–å‡ºæ—¶çš„äº¤æ˜“æˆæœ¬æ¯”ç‡ã€‚

        è¿”å›:
        å¥–åŠ±å€¼ã€‚
        """

        # è®¡ç®—ä½ç½®çŠ¶æ€çš„å˜åŒ–
        position_change = sa_next - sa_current

            # å¦‚æœä¸‹ä¸€çŠ¶æ€å¤§äºå½“å‰çŠ¶æ€ï¼ˆä»£è¡¨ä¹°å…¥ï¼‰
        if position_change > 0:
            reward = -abs(position_change) * (1 + alpha_buy) * p_buy
            # å¦åˆ™ï¼ˆä»£è¡¨å–å‡ºæˆ–ä¿æŒä¸å˜ï¼‰
        else:
            reward = abs(position_change) * (1 - alpha_sell) * p_sell

        return reward        
        
class Env():
    def __init__(self,reset_on_close = False, random_ofs_on_reset=True) -> None:
        """
            ç”¨ä¾†å»ºæ§‹å®Œæ•´ç’°å¢ƒ
            
        """        
        self._state = State(bars_count=50,reset_on_close=reset_on_close)
        self._prices = DataFeature().get_train_net_work_data()
        self.random_ofs_on_reset = random_ofs_on_reset
        
    def reset(self):
        """
            å•†å“éš¨æ©Ÿæ€§
            èµ·æ­¥éš¨æ©Ÿæ€§
        """        
        self._instrument = np.random.choice(list(self._prices.keys()))
        prices = self._prices[self._instrument]

        bars = self._state.bars_count
        
        if self.random_ofs_on_reset:
            offset = np.random.choice(prices.high.shape[0]-bars*10) + bars
        else:
            offset = bars

        print("æ­¤æ¬¡æ­¥æ•¸ç‚º:",offset)
        self._state.reset(prices, offset)
        return self._state.encode()
    
    
    def step(self,action_idx):
        """
            retunr : 
                oberservation_ (ä¸‹ä¸€å€‹è§€å¯Ÿç‹€æ…‹),
                reward(çå‹µ),
                done(æ˜¯å¦å®Œæˆ),
                info(å…¶ä»–è³‡æ–™),
        """
        
        """
            å‘¼å«å­é¡_state ä¾†ç²å¾—çå‹µ
        Args:
            action_idx (_type_): _description_

        Returns:
            _type_: _description_
        """
        action = Actions(action_idx)


        reward, done = self._state.step(action) # é€™é‚Šæœƒæ›´æ–°æ­¥æ•¸
        obs = self._state.encode() # å‘¼å«é€™è£¡çš„æ™‚å€™å°±æœƒå–å¾—æ–°çš„ç‹€æ…‹
        info = {"instrument": self._instrument, "offset": self._state._offset}
        return obs, reward, done, info
    
    
    def update_alpha(self,step_idx):
        self._state.update_alpha(step_idx)